{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVBoQ-w_ccqO"
      },
      "source": [
        "# [Ερώτημα 1: Πράξεις με Διανύσματα και Πίνακες]\n",
        "\n",
        "Αρχικά δημιουργούμε:\n",
        "\n",
        "i) Δύο τυχαίους πίνακες ακέραιων αριθμών $X \\in ℤ^{3x4}, Y \\in ℤ^{4x3}$ και\n",
        "\n",
        "ii) Δύο τυχαία διανύσματα ακέραιων αριθμών $a \\in ℤ^{4}, b \\in ℤ^{4}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma1Wlx3Nz3rQ",
        "outputId": "aeef783b-7e25-4eef-d934-f77001464e75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X:\n",
            "[[8 6 7 9]\n",
            " [6 0 8 9]\n",
            " [7 6 0 7]]\n",
            "Y:\n",
            "[[4 0 6]\n",
            " [4 5 5]\n",
            " [8 5 9]\n",
            " [8 7 3]]\n",
            "a: [4 8 1 6]\n",
            "b: [0 9 5 6]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(14)                    # seed = final 4 digits of AM\n",
        "X = np.random.randint(0,10, (3,4))    # random numpy array of shape (3,4)\n",
        "Y = np.random.randint(0,10, (4,3))    # random numpy array of shape (4,3)\n",
        "\n",
        "a = np.random.randint(0,10,4)         #vector row (random numpy array of shape row)\n",
        "b = np.random.randint(0,10,4)\n",
        "\n",
        "print (\"X:\")\n",
        "print(X)\n",
        "print (\"Y:\")\n",
        "print(Y)\n",
        "print (\"a:\", end=\" \")\n",
        "print(a)\n",
        "print (\"b:\", end=\" \")\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM47ynny-MRf"
      },
      "source": [
        "#(1.1)\n",
        "Το εσωτερικό γινόμενο των a=(4,8,1,6), b=(0,9,5,6) θα είναι:\n",
        "\n",
        "$a^Tb = \n",
        " \\begin{pmatrix}\n",
        " 4\\\\8\\\\1\\\\6\n",
        " \\end{pmatrix}\\begin{pmatrix}0,9,5,6\\end{pmatrix}\n",
        " = 4 * 0 + 8 * 9 + 1 * 5 + 6 * 6 = 0 + 72 + 5 + 36 = 113 \\\\ $\n",
        "\n",
        "#(1.2)\n",
        "Το γινόμενο πίνακα-διανύσματος των $Xa \\in Z^3$ θα είναι:\n",
        "\n",
        "$Xa = \n",
        " \\begin{pmatrix}\n",
        "  8 & 6 & 7 & 9 \\\\\n",
        "  6 & 0 & 8 & 9 \\\\\n",
        "  7  & 6  & 0 & 7\n",
        " \\end{pmatrix}\\begin{pmatrix}4,8,1,6\\end{pmatrix}=\n",
        "\\begin{pmatrix}8\\cdot4+6\\cdot8+7\\cdot1+9\\cdot6,\\quad6\\cdot4+0\\cdot8+8\\cdot1+9\\cdot6,\\quad7\\cdot4+6\\cdot8+0\\cdot1+7\\cdot6\\end{pmatrix}=(141,86,118) \\\\ $\n",
        "\n",
        "#(1.3)\n",
        "Το γινόμενο των πινάκων $XY \\in Z^{3x3}$ θα είναι:\n",
        "\n",
        "$XY = \n",
        " \\begin{pmatrix}\n",
        "  8 & 6 & 7 & 9 \\\\\n",
        "  6 & 0 & 8 & 9 \\\\\n",
        "  7  & 6  & 0 & 7\n",
        " \\end{pmatrix}\n",
        " \\begin{pmatrix}\n",
        "  4 & 0 & 6 \\\\\n",
        "  4 & 5 & 5 \\\\\n",
        "  8 & 5 & 9 \\\\\n",
        "  8 & 7 & 3\n",
        " \\end{pmatrix}= \n",
        " \\begin{pmatrix}\n",
        "  8\\cdot4+6\\cdot4+7\\cdot8+9\\cdot8 & 8\\cdot0+6\\cdot5+7\\cdot5+9\\cdot7 & 8\\cdot6+6\\cdot5+7\\cdot9+9\\cdot3 \\\\\n",
        "  6\\cdot4+0\\cdot4+8\\cdot8+9\\cdot8 & 6\\cdot0+0\\cdot5+8\\cdot5+9\\cdot7 & 6\\cdot6+0\\cdot5+8\\cdot9+9\\cdot3 \\\\\n",
        "  7\\cdot4+6\\cdot4+0\\cdot8+7\\cdot8 & 7\\cdot0+6\\cdot5+0\\cdot5+7\\cdot7 & 7\\cdot6+6\\cdot5+0\\cdot9+7\\cdot3\n",
        " \\end{pmatrix}=\n",
        " \\begin{pmatrix}\n",
        "  184 & 128 & 168 \\\\\n",
        "  160 & 103 & 135 \\\\\n",
        "  108 & 79 & 93\n",
        " \\end{pmatrix}\n",
        "\\\\ $\n",
        "\n",
        "\n",
        "#(1.4)\n",
        "Η Ευκλείδεια νόρμα (l2-norm) του διανύσματος a θα είναι:\n",
        "\n",
        "$||a||_2 = \\sqrt{a_1^2 + a_2^2 + a_3^2 + a_4^2} = \\sqrt{4^2 + 8^2 + 1^2 + 6^2} = \\sqrt{16 + 64 + 1 + 36} = \\sqrt{117} = 10.8166538 \\\\ $\n",
        "\n",
        "#(1.5)\n",
        "Η  Frobenius νόρμα του πίνακα X θα είναι:\n",
        "\n",
        "$||X||_F = \\sqrt{X_{11}^2 + X_{21}^2 + X_{31}^2 + X_{12}^2 + X_{22}^2 + X_{32}^2 + X_{13}^2 + X_{23}^2 + X_{33}^2 + X_{14}^2 + X_{24}^2 + X_{34}^2} = \n",
        "\\sqrt{8^2 + 6^2 + 7^2 + 6^2 + 0^2 + 6^2 + 7^2 + 8^2 + 0^2 + 9^2 + 9^2 + 7^2} = \n",
        "\\sqrt{64 + 36 + 49 + 36 + 36 + 49 + 64 + 81 + 81 + 49} = \\sqrt{545} = 23.3452351$\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWsjkMJrgbuK"
      },
      "source": [
        "# Επαληθεύουμε τις μαθηματικές λύσεις μέσω της numPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChJMLizS-PId"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(14)                    # seed = final 4 digits of AM\n",
        "X = np.random.randint(0,10, (3,4))    # random numpy array of shape (3,4)\n",
        "Y = np.random.randint(0,10, (4,3))    # random numpy array of shape (4,3)\n",
        "\n",
        "a = np.random.randint(0,10,4)         #vector row (random numpy array of shape row)\n",
        "b = np.random.randint(0,10,4)\n",
        "\n",
        "print (\"(Question 1.1)\")\n",
        "print(\"ab = \", end=\"\")\n",
        "print(np.inner(a,b))\n",
        "\n",
        "print (\"\\n(Question 1.2)\")\n",
        "print(\"Xa = \", end=\"\")\n",
        "print(X.dot(a))\n",
        "\n",
        "print (\"\\n(Question 1.3)\")\n",
        "print(\"XY =\")\n",
        "print(X.dot(Y))\n",
        "\n",
        "print (\"\\n(Question 1.4)\")\n",
        "print(\"l2-norm of a = ||a||_2 = \", end=\"\")\n",
        "print(np.linalg.norm(a))\n",
        "\n",
        "print (\"\\n(Question 1.5)\")\n",
        "print(\"frobenius norm of X = ||X||_F = \", end=\"\")\n",
        "print(np.linalg.norm(X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkr_Dgk7cnwg"
      },
      "source": [
        "# [Ερώτημα 2: Υπολογισμός παραγώγων]\n",
        "\n",
        "# (2.1)\n",
        "Συνάρτηση: $f(x) = x^TAx + b^Tx $\n",
        "\n",
        "Η παράγωγος μιας συνάρτησης $f:R\\rightarrow R^d, \\nabla f(x)$, ως προς το διάνυσμα $x \\in R^d$ είναι:\n",
        "$\\nabla f(x)=\\begin{pmatrix} \\frac{\\partial f}{\\partial  x_1}  \\\\ \\vdots\\\\ \\frac{\\partial f}{\\partial  x_d} \\end{pmatrix} \\in R^d$\n",
        "\n",
        "Θεωρούμε ότι:\n",
        "\n",
        "$g(x) = x^TAx$ και $h(x) = b^Tx$\n",
        "\n",
        "$\\partial f(x) = \\partial g(x) + \\partial h(x)$\n",
        "\n",
        "Για την $g(x)$:\n",
        "\n",
        "\\begin{align}\n",
        "y & = g(x) \\\\\n",
        "& = x^TAx \\\\\n",
        "& = \\begin{pmatrix}x1  \\\\ \\vdots\\\\ x_d\\end{pmatrix} \\begin{pmatrix} a_{11} & a_{12} & \\dots & a_{1d}\\\\  \\vdots & \\vdots &\\ddots & \\vdots \\\\ a_{d1} & a_{d2} & \\dots & a_{dd} \\end{pmatrix} \\begin{pmatrix}x1 \\dots x_d\\end{pmatrix}\\\\\n",
        "& = \\sum_{i=1}^d\\sum_{j=1}^d a_{ij}x_ix_j \\\\\n",
        "& = \\sum_{i=1}^da_{i1}x_ix_1+\\sum_{j=1}^da_{1j}x_1x_j+\\sum_{i=2}^d\\sum_{j=2}^d a_{ij}x_ix_j\\\\\n",
        "\\text{Άρα έχουμε:} \\\\\n",
        "\\frac{\\partial g}{\\partial  x_1} & = \\sum_{i=1}^da_{i1}x_i+\\sum_{j=1}^da_{1j}x_j \\\\\n",
        "& = \\sum_{i=1}^da_{1i}x_i+\\sum_{i=1}^da_{1i}x_i \\,[\\text{αφού} \\; a_{1i}=a_{1i}]\\\\\n",
        "& = 2\\sum_{i=1}^da_{1i}x_i \\\\ \n",
        "\\frac{\\partial g}{\\partial  x} \n",
        "& = \\begin{pmatrix} 2\\sum_{i=1}^na_{1i}x_i \\\\ \\vdots\\\\ 2 \\sum_{i=1}^na_{ni}x_i \\end{pmatrix} \\\\\n",
        "& = 2\\begin{pmatrix} a_{11} & a_{12} & \\dots & a_{1d}\\\\  \\vdots & \\vdots &\\ddots & \\vdots \\\\ a_{d1} & a_{d2} & \\dots & a_{dd} \\end{pmatrix}\n",
        "\\begin{pmatrix}x_1  \\\\ \\vdots \\\\ x_n \\end{pmatrix}\\\\ \n",
        "& = 2Ax\n",
        "\\end{align}\n",
        "\n",
        "Για την $h(x)$:\n",
        "\n",
        "\\begin{align}\n",
        "y & = h(x) \\\\\n",
        "& = b^Tx \\\\\n",
        "& = \\begin{pmatrix}b1  \\\\ \\vdots\\\\ b_d\\end{pmatrix}\\begin{pmatrix}x1 \\dots x_d\\end{pmatrix}\\\\\n",
        "& = \\sum_{i=1}^d b_i x_i \\\\\n",
        "& =  b_1 x_1 + \\sum_{i=2}^d b_i x_i \\\\\n",
        "\\text{Άρα έχουμε:} \\\\\n",
        "\\frac{\\partial h}{\\partial  x_1} & = b1\\\\ \n",
        "\\frac{\\partial h}{\\partial  x} \n",
        "& = \\begin{pmatrix} b_1 \\\\ b_2\\\\ \\vdots\\\\ b_d\\end{pmatrix} \\\\\n",
        "& = b^T\n",
        "\\end{align}\n",
        "\n",
        "Συνολικά\n",
        "\n",
        "Η παράγωγος της συνάρτησης $f$ ως προς το διάνυσμα $x$ είναι:\n",
        "\n",
        "$\\nabla f(x) = 2Ax + b^Τ $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUoSZxEz5M8I"
      },
      "source": [
        "# (2.2)\n",
        "\n",
        "Για να βρούμε, αναλυτικά και σε κλειστή μορφή (closed form solution) το ολικό ελάχιστο του προβλήματος βελτιστοποίησης:\n",
        "\n",
        "Θέτουμε:\n",
        "$U = A-XB$ και $f = \\|U\\|_F^2$\n",
        "\n",
        "Από matrix cookbook γνωρίζουμε ότι $\\|U\\|_F^2 = Tr[U \\cdot U^T]$ οπότε:\n",
        "\n",
        "\\begin{align}\n",
        "\\|A-XB\\|_F^2 &= Tr[(A-XB) \\cdot (A-XB)^T]\\\\\n",
        "\\end{align}\n",
        "\n",
        "Παραγωγίζουμε γνωρίζοντας από matrix cookbook ότι $\\frac{\\partial Tr(F(X))}{\\partial X} = f(X)^T \\\\ $:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial X}Tr[(A-XB) \\cdot (A-XB)^T] &= \\frac{\\partial}{\\partial X}Tr[(A-XB) \\cdot (A^T-X^TB^T)]\\\\\n",
        "&= \\frac{\\partial}{\\partial X}Tr[AA^T-AX^TB^T-XBA^T - XBX^TB^T]\\\\\\\\\n",
        "&= (-A^ΤB - BA^T + 2X^TBB^T)^T \\\\\n",
        "&= (-2B(A^T-X^TB^T))^T\\\\\n",
        "&= -2B^T(A-XB)\n",
        "\\end{align}\n",
        "\n",
        "Θέτουμε την παράγωγο ίση με 0 και λύνουμε ως προς $X$:\n",
        "\n",
        "\\begin{align}\n",
        "-2B^T(A-XB) &= 0\\\\\n",
        "2B^TXB &= 2B^TA\\\\\n",
        "X &= \\frac{A}{B}\n",
        "\\end{align}\n",
        "\n",
        "Τελικά: \n",
        "\\begin{align}\n",
        "min \\|A-XB\\|_F^2 &= \\|A-\\frac{A}{B}B\\|_F^2\\\\\n",
        "&= 0\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtY_140yB9lQ"
      },
      "source": [
        "# [Ερώτημα 3: Gradient descent]\n",
        "\n",
        "#Για τη συνάρτηση: $f_1([x_1, x_2]^T) = f_1(x_1, x_2) = (x_1 − 2)^2 + (x_2 − 3)^2$\n",
        "\n",
        "Ο αλγόριθμος βρίσκει το global minimum της f σε δύο επαναλήψεις για learning rate = 0.5, ενώ για μικρότερα learning rates καταλήγει στο global minimum με επαναλήψεις που αυξάνονται όσο μικραίνει το learning rate. Παρακάτω δοκιμάζω τον αλγόριθμο για τη συνάρτηση με precision = 0.00001 για:\n",
        "\n",
        "learning rate = 0.5, maxIter = 10 και \n",
        "\n",
        "learning rate = 0.2, maxIter = 20\n",
        "\n",
        "και εμφανίζω γραφήματα που να απεικονίζουν την τιμή της συνάρτησης (κάθετος άξονας) ως συνάρτηση των επαναλήψεων (οριζόντιος άξονας)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3-Tuv_vF8Bb"
      },
      "outputs": [],
      "source": [
        "##############################################################################################################################\n",
        "\n",
        "from sympy import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x1 = Symbol('x1')\n",
        "x2 = Symbol('x2')\n",
        "\n",
        "f = (x1 - 2)**2 + (x2 - 3)**2\n",
        "\n",
        "# First partial derivative with respect to x1\n",
        "fx1 = f.diff(x1)\n",
        "# fpx1 = 2*x1 - 4\n",
        "\n",
        "# First partial derivative with respect to x2\n",
        "fx2 = f.diff(x2)\n",
        "# fpx2 = 2*x2 - 6\n",
        "\n",
        "gradient = [fx1,fx2]\n",
        "x = 0\n",
        "y = 0\n",
        "lrate = 0.5\n",
        "numIter = 0     \n",
        "maxIter = 10\n",
        "precision = 0.001\n",
        "\n",
        "T = []\n",
        "F = []\n",
        "\n",
        "while True:\n",
        "  tempx = x - lrate * fx1.subs(x1,x).subs(x2,y).evalf()\n",
        "  tempy = y - lrate * fx2.subs(x1,x).subs(x2,y).evalf()\n",
        "\n",
        "  numIter += 1\n",
        "  if numIter > maxIter:\n",
        "    break\n",
        "\n",
        "  #If the values of x1/x2 change less than the amount of precision, break\n",
        "  if abs(tempx-x) < precision and abs(tempy-y) < precision:\n",
        "    converge = True\n",
        "    break\n",
        "\n",
        "  x = tempx\n",
        "  y = tempy\n",
        "  print(\"Iteration\",numIter)\n",
        "  print(\"f value:\",f.subs(x1,x).subs(x2,y).evalf())\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "\n",
        "if converge:\n",
        "  print(\"\\nNumber of iterations:\",numIter)\n",
        "  print(\"f converges to a minimum:\", int(round(f.subs(x1,tempx).subs(x2,tempy).evalf())))\n",
        "  print(\"x =\",tempx,sep=\" \")\n",
        "  print(\"y =\",tempy,sep=\" \")\n",
        "\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "  plt.plot(T,F)\n",
        "  plt.xlabel(\"Iterations (T)\")\n",
        "  plt.ylabel(\"Value of Function (F)\")\n",
        "  plt.title(\"Learning Rate = \"+str(lrate))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtg34gjJheUE"
      },
      "outputs": [],
      "source": [
        "##############################################################################################################################\n",
        "\n",
        "from sympy import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x1 = Symbol('x1')\n",
        "x2 = Symbol('x2')\n",
        "\n",
        "f = (x1 - 2)**2 + (x2 - 3)**2\n",
        "\n",
        "# First partial derivative with respect to x1\n",
        "fx1 = f.diff(x1)\n",
        "# fpx1 = 2*x1 - 4\n",
        "\n",
        "# First partial derivative with respect to x2\n",
        "fx2 = f.diff(x2)\n",
        "# fpx2 = 2*x2 - 6\n",
        "\n",
        "gradient = [fx1,fx2]\n",
        "x = 0\n",
        "y = 0\n",
        "lrate = 0.2\n",
        "numIter = 0     \n",
        "maxIter = 20\n",
        "precision = 0.001\n",
        "\n",
        "T = []\n",
        "F = []\n",
        "\n",
        "while True:\n",
        "  tempx = x - lrate * fx1.subs(x1,x).subs(x2,y).evalf()\n",
        "  tempy = y - lrate * fx2.subs(x1,x).subs(x2,y).evalf()\n",
        "\n",
        "  numIter += 1\n",
        "  if numIter > maxIter:\n",
        "    break\n",
        "\n",
        "  #If the values of x1/x2 change less than the amount of precision, break\n",
        "  if abs(tempx-x) < precision and abs(tempy-y) < precision:\n",
        "    converge = True\n",
        "    break\n",
        "\n",
        "  x = tempx\n",
        "  y = tempy\n",
        "  print(\"Iteration\",numIter)\n",
        "  print(\"f value:\",f.subs(x1,x).subs(x2,y).evalf())\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "\n",
        "if converge:\n",
        "  print(\"\\nNumber of iterations:\",numIter)\n",
        "  print(\"f converges to a minimum:\", int(round(f.subs(x1,tempx).subs(x2,tempy).evalf())))\n",
        "  print(\"x =\",tempx,sep=\" \")\n",
        "  print(\"y =\",tempy,sep=\" \")\n",
        "\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "  plt.plot(T,F)\n",
        "  plt.xlabel(\"Iterations (T)\")\n",
        "  plt.ylabel(\"Value of Function (F)\")\n",
        "  plt.title(\"Learning Rate = \"+str(lrate))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1Hy5ANmfLb9"
      },
      "source": [
        "#Για τη συνάρτηση: $f_2([x_1, x_2]^T) = f_2(x_1, x_2) = (1 − (x_2 − 3))^2 + 20 ((x_1 + 3) − (x_2 − 3)^2)^2$\n",
        "\n",
        "Ο αλγόριθμος δεν βρίσκει το global minimum της f για τιμές του learning rate > 0.005. Αυτό συμβαίνει καθώς η συνάρτηση είναι \"δύσκολη\", επομένως με μεγάλα βήματα είναι πολύ εύκολο να κάνει ο αλγόριθμος λάθος και η τιμή της f να κάνει diverge αντί για converge. \n",
        "\n",
        "Βρήκα το \"καλύτερο\" αποτέλεσμα (ελάχιστες επαναλήψεις) για learning rate = 0.0042 όπου και καταλήγει σε global minimum σε 3254 επαναλήψεις (precision = 0.00001)\n",
        "\n",
        "Παρακάτω δοκιμάζω τον αλγόριθμο για τη συνάρτηση με precision = 0.00001 για:\n",
        "\n",
        "learning rate = 0.5, maxIter = 10 \n",
        "\n",
        "learning rate = 0.0042, maxIter = 5000 \n",
        "\n",
        "learning rate = 0.002, maxIter = 10000\n",
        "\n",
        "και εμφανίζω γραφήματα που να απεικονίζουν την τιμή της συνάρτησης (κάθετος άξονας) ως συνάρτηση των επαναλήψεων (οριζόντιος άξονας). Για καλύτερα γραφήματα έχω βάλει σε σχόλιο το precision που αναδεικνύει καλύτερα το γράφημα για κάθε παράδειγμα (Προφανώς δεν καταλήγουμε σε τέλεια λύση για τα x1,x2,f) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcM-95ALnQyE"
      },
      "outputs": [],
      "source": [
        "##############################################################################################################################\n",
        "\n",
        "from sympy import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x1 = Symbol('x1')\n",
        "x2 = Symbol('x2')\n",
        "\n",
        "f = (1-(x2-3))**2 + 20*((x1+3)-(x2-3)**2)**2\n",
        "\n",
        "# First partial derivative with respect to x1\n",
        "fx1 = f.diff(x1)\n",
        "# fx1 = 40*x1 - 40*(x2 - 3)**2 + 120\n",
        "\n",
        "# First partial derivative with respect to x2\n",
        "fx2 = f.diff(x2)\n",
        "# fx2 = 2*x2 + 20*(12 - 4*x2)*(x1 - (x2 - 3)**2 + 3) - 8\n",
        "\n",
        "\n",
        "gradient = [fx1,fx2]\n",
        "x = 0\n",
        "y = 0\n",
        "lrate = 0.5\n",
        "numIter = 0\n",
        "maxIter = 10\n",
        "precision = 0.00001\n",
        "converge = False\n",
        "\n",
        "T = []\n",
        "F = []\n",
        "\n",
        "while True:\n",
        "  tempx = x - lrate * fx1.subs(x1,x).subs(x2,y).evalf()\n",
        "  tempy = y - lrate * fx2.subs(x1,x).subs(x2,y).evalf()\n",
        "\n",
        "  numIter += 1\n",
        "  if numIter > maxIter:\n",
        "    break\n",
        "\n",
        "  #If the values of x1/x2 change less than the amount of precision, break\n",
        "  if abs(tempx-x) < precision and abs(tempy-y) < precision:\n",
        "    converge = True\n",
        "    break\n",
        "\n",
        "  x = tempx\n",
        "  y = tempy\n",
        "  print(\"Iteration\",numIter)\n",
        "  print(\"f value:\",f.subs(x1,x).subs(x2,y).evalf())\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "\n",
        "if converge:\n",
        "  print(\"\\nNumber of iterations:\",numIter)\n",
        "  print(\"f converges to a minimum:\", int(round(f.subs(x1,tempx).subs(x2,tempy).evalf())))\n",
        "  print(\"x =\",tempx,sep=\" \")\n",
        "  print(\"y =\",tempy,sep=\" \")\n",
        "\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "  plt.plot(T,F)\n",
        "  plt.xlabel(\"Iterations (T)\")\n",
        "  plt.ylabel(\"Value of Function (F)\")\n",
        "  plt.title(\"Learning Rate = \"+str(lrate))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1COxSYyNc-r"
      },
      "outputs": [],
      "source": [
        "##############################################################################################################################\n",
        "\n",
        "from sympy import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x1 = Symbol('x1')\n",
        "x2 = Symbol('x2')\n",
        "\n",
        "f = (1-(x2-3))**2 + 20*((x1+3)-(x2-3)**2)**2\n",
        "\n",
        "# First partial derivative with respect to x1\n",
        "fx1 = f.diff(x1)\n",
        "# fx1 = 40*x1 - 40*(x2 - 3)**2 + 120\n",
        "\n",
        "# First partial derivative with respect to x2\n",
        "fx2 = f.diff(x2)\n",
        "# fx2 = 2*x2 + 20*(12 - 4*x2)*(x1 - (x2 - 3)**2 + 3) - 8\n",
        "\n",
        "\n",
        "gradient = [fx1,fx2]\n",
        "x = 0\n",
        "y = 0\n",
        "lrate = 0.0042\n",
        "numIter = 0\n",
        "maxIter = 5000\n",
        "precision = 0.00001                #0.01\n",
        "converge = False\n",
        "\n",
        "T = []\n",
        "F = []\n",
        "printCount = 0\n",
        "\n",
        "while True:\n",
        "  tempx = x - lrate * fx1.subs(x1,x).subs(x2,y).evalf()\n",
        "  tempy = y - lrate * fx2.subs(x1,x).subs(x2,y).evalf()\n",
        "\n",
        "  numIter += 1\n",
        "  if numIter > maxIter:\n",
        "    break\n",
        "\n",
        "  #If the values of x1/x2 change less than the amount of precision, break\n",
        "  if abs(tempx-x) < precision and abs(tempy-y) < precision:\n",
        "    converge = True\n",
        "    break\n",
        "\n",
        "  x = tempx\n",
        "  y = tempy\n",
        "  printCount += 1\n",
        "  if printCount == 300:\n",
        "    print(\"Iteration\",numIter)\n",
        "    print(\"f value:\",f.subs(x1,x).subs(x2,y).evalf())\n",
        "    printCount = 0\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "\n",
        "if converge:\n",
        "  print(\"\\nNumber of iterations:\",numIter)\n",
        "  print(\"f converges to a minimum:\", int(round(f.subs(x1,tempx).subs(x2,tempy).evalf())))\n",
        "  print(\"x =\",tempx,sep=\" \")\n",
        "  print(\"y =\",tempy,sep=\" \")\n",
        "\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "  plt.plot(T,F)\n",
        "  plt.xlabel(\"Iterations (T)\")\n",
        "  plt.ylabel(\"Value of Function (F)\")\n",
        "  plt.title(\"Learning Rate = \"+str(lrate))\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsMaXJV8kaJo"
      },
      "outputs": [],
      "source": [
        "##############################################################################################################################\n",
        "\n",
        "from sympy import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x1 = Symbol('x1')\n",
        "x2 = Symbol('x2')\n",
        "\n",
        "f = (1-(x2-3))**2 + 20*((x1+3)-(x2-3)**2)**2\n",
        "\n",
        "# First partial derivative with respect to x1\n",
        "fx1 = f.diff(x1)\n",
        "\n",
        "# First partial derivative with respect to x2\n",
        "fx2 = f.diff(x2)\n",
        "\n",
        "gradient = [fx1,fx2]\n",
        "x = 0\n",
        "y = 0\n",
        "lrate = 0.002\n",
        "numIter = 0\n",
        "maxIter = 10000\n",
        "precision = 0.00001            #0.004\n",
        "converge = False\n",
        "\n",
        "T = []\n",
        "F = []\n",
        "printCount = 0\n",
        "\n",
        "while True:\n",
        "  tempx = x - lrate * fx1.subs(x1,x).subs(x2,y).evalf()\n",
        "  tempy = y - lrate * fx2.subs(x1,x).subs(x2,y).evalf()\n",
        "\n",
        "  numIter += 1\n",
        "  if numIter > maxIter:\n",
        "    break\n",
        "\n",
        "  #If the values of x1/x2 change less than the amount of precision, break\n",
        "  if abs(tempx-x) < precision and abs(tempy-y) < precision:\n",
        "    converge = True\n",
        "    break\n",
        "\n",
        "  x = tempx\n",
        "  y = tempy\n",
        "  printCount += 1\n",
        "  if printCount == 300:\n",
        "    print(\"Iteration\",numIter)\n",
        "    print(\"f value:\",f.subs(x1,x).subs(x2,y).evalf())\n",
        "    printCount = 0\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "\n",
        "if converge:\n",
        "  print(\"Number of iterations:\",numIter)\n",
        "  print(\"f converges to a minimum:\", int(round(f.subs(x1,tempx).subs(x2,tempy).evalf())))\n",
        "  print(\"x =\",tempx,sep=\" \")\n",
        "  print(\"y =\",tempy,sep=\" \")\n",
        "\n",
        "  T.append(numIter)\n",
        "  F.append(f.subs(x1,x).subs(x2,y).evalf())\n",
        "  plt.plot(T,F)\n",
        "  plt.xlabel(\"Iterations (T)\")\n",
        "  plt.ylabel(\"Value of Function (F)\")\n",
        "  plt.title(\"Learning Rate = \"+str(lrate))\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrTAXf3luH1-"
      },
      "source": [
        "# Συμπέρασμα:\n",
        "\n",
        "Όπως φαίνεται από το παράδειγμα της πρώτης συνάρτησης και ιδιαίτερα στο παράδειγμα της δεύτερης συνάρτησης που είναι πιο \"δύσκολη\" για τον αλγόριθμο, η τιμή του learning rate είναι αντιστρόφως ανάλογη του αριθμού των επαναλήψεων, είναι όμως ανάλογη της πιθανότητας λάθους.\n",
        "\n",
        "Επομένως, για κάθε συνάρτηση, ανάλογα με τη δυσκολία της είναι θεμιτό να βρίσκουμε learning rate το οποίο ελαχιστοποιεί τον αριθμό επαναλήψεων μέχρι να φτάσουμε στο global minimum της συνάρτησης με αρκετή ακρίβεια (precision), ΧΩΡΙΣ όμως αυτό το learning rate να συγκλίνει πρώιμα σε λάθος σημείο ή να προκαλεί απόκλιση (divergence) στη συνάρτηση μας. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Vavoulas-1115201800014",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
